{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "`feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:23:39] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     inst_id       XGB       GBM   RF       ens  OC\n",
      "1          5  0.655411  0.443177  0.8  0.632863   0\n",
      "2          6  0.409096  0.666447  0.6  0.558514   0\n",
      "13        30  0.409096  0.230173  0.8  0.479756   0\n",
      "22        64  0.409096  0.936367  0.1  0.481821   0\n",
      "78       229  0.409096  0.539125  0.6  0.516074   0\n",
      "84       258  0.409096  0.442562  0.6  0.483886   0\n",
      "88       293  0.409096  0.724379  0.7  0.611158   0\n",
      "99       341  0.409096  0.278068  0.4  0.362388   0\n",
      "120      413  0.409096  0.278068  0.4  0.362388   0\n",
      "122      424  0.409096  0.312302  0.3  0.340466   0\n",
      "123      425  0.409096  0.580902  0.7  0.563333   0\n",
      "124      429  0.409096  0.536108  0.7  0.548401   0\n",
      "126      431  0.409096  0.536108  0.6  0.515068   0\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Sep 18 19:03:22 2018\n",
    "\n",
    "@author: BTHANISH\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Reading the train and test files\n",
    "train_prod_df = pd.read_csv('data/train.csv')\n",
    "test_prod_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "#Removing the comma in the employee1 and 2 columns in the test dataset and replace it with empty space and convert it to float format.\n",
    "test_prod_df.employee1 = test_prod_df.employee1.astype('str').str.replace(\",\", \"\").astype('float')\n",
    "test_prod_df.employee2 = test_prod_df.employee2.astype('str').str.replace(\",\", \"\").astype('float')\n",
    "\n",
    "#Converting the employee1 and 2 column as float in the train set as done for the test dataset\n",
    "train_prod_df.employee1 = train_prod_df.employee1.astype('float')\n",
    "train_prod_df.employee2 = train_prod_df.employee2.astype('float')\n",
    "train_prod_df.OC= train_prod_df.OC.astype('str').str.replace(\" \",\"\")\n",
    "\n",
    "#Combining the train and test dataset\n",
    "train_test_prod = train_prod_df.append(test_prod_df)\n",
    "\n",
    "#Get the object and numeric columns seperately \n",
    "factor_columns = train_test_prod.select_dtypes(include = ['object']).columns\n",
    "numeric_columns = train_test_prod.columns.difference(factor_columns)\n",
    "\n",
    "#After analysis realized that the bed counts of these two hospitals may have had wrong entries.\n",
    "#Filling up the empty instkind and bedCount for hospital id 430 and 413\n",
    "train_test_prod.loc[train_test_prod.inst_id == 430, ['instkind']] = 'dental_clinic'\n",
    "train_test_prod.loc[train_test_prod.inst_id == 430, ['bedCount']] = 0\n",
    "train_test_prod.loc[train_test_prod.inst_id == 413, ['bedCount']] = -999\n",
    "\n",
    "#Fill the empty values in the object columns as \"Not sure\"\n",
    "train_test_prod[factor_columns] = train_test_prod[factor_columns].fillna('Not_sure')\n",
    "\n",
    "#Fill all the empty values in the numeric columns as -999\n",
    "train_test_prod[numeric_columns] = train_test_prod[numeric_columns].fillna(-999)\n",
    "\n",
    "#Convert all the object columns to numeric since the ML algorithms don't accept object features directly \n",
    "fac_le = LabelEncoder()\n",
    "train_test_prod[factor_columns] = train_test_prod.loc[:,factor_columns].apply(lambda x : fac_le.fit_transform(x))\n",
    "\n",
    "#Splitting back data to train prod and test prod\n",
    "train_prod = train_test_prod.loc[train_test_prod.OC != 0,]\n",
    "test_prod = train_test_prod.loc[train_test_prod.OC == 0,]\n",
    "train_prod['OC'] = train_prod['OC'] - 1\n",
    "\n",
    "#Obtain the submission ID to create the submission file later\n",
    "sub_id = test_prod.inst_id\n",
    "\n",
    "#Get the dependent and independent column\n",
    "dep = 'OC'\n",
    "indep = train_prod.columns.difference([dep])\n",
    "\n",
    "\n",
    "train_prod_X = train_prod[indep]\n",
    "train_prod_Y = train_prod[dep]\n",
    "test_prod_X = test_prod[indep]\n",
    "\n",
    "############################################################################\n",
    "############ Random Forest\n",
    "############################################################################\n",
    "estimators = 10\n",
    "np.random.seed(100)\n",
    "RF_prod = RandomForestClassifier(n_estimators = estimators)\n",
    "RF_prod_model = RF_prod.fit(train_prod_X, train_prod_Y)\n",
    "RF_prod_prediction = RF_prod.predict_proba(test_prod_X)[:,1]\n",
    "\n",
    "sub_RF = pd.DataFrame({'inst_id' : sub_id , 'OC' : RF_prod_prediction })\n",
    "sub_RF = sub_RF[['inst_id', 'OC']]\n",
    "############################################################################\n",
    "############ GBM\n",
    "############################################################################\n",
    "estimators = 10\n",
    "np.random.seed(100)\n",
    "GBM_prod = GradientBoostingClassifier(n_estimators = estimators)\n",
    "GBM_prod_model = GBM_prod.fit(train_prod_X, train_prod_Y)\n",
    "GBM_prod_prediction = GBM_prod.predict_proba(test_prod_X)[:,1]\n",
    "\n",
    "sub_GBM = pd.DataFrame({'inst_id' : sub_id , 'OC' : GBM_prod_prediction })\n",
    "sub_GBM = sub_GBM[['inst_id', 'OC']]\n",
    "############################################################################\n",
    "############ XGBOOST\n",
    "############################################################################\n",
    "dtrain_prod = xgb.DMatrix(data = train_prod_X, label = train_prod_Y)\n",
    "dtest_prod = xgb.DMatrix(data = test_prod_X)\n",
    "\n",
    "#Custom error function for the XGB model\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def best_threshold_f1(preds, labels):\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    f1_scores = [f1_score(labels, (preds > thr).astype('float')) for thr in thresholds]\n",
    "    return thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "def eval_error(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = (preds > 0.5).astype('float')\n",
    "    return \"f1_score\", f1_score(labels, preds)\n",
    "    \n",
    "\n",
    "param = {'objective' : 'binary:logistic',\n",
    "         'max_depth' : 6,\n",
    "         'eta': 0.3,\n",
    "         'colsample_bytree' : 1,\n",
    "         'subsample' : 1,\n",
    "         'silent' : 0\n",
    "         }\n",
    "\n",
    "nrounds = 2\n",
    "np.random.seed(100)\n",
    "xgb_model = xgb.train(param, \n",
    "                      dtrain_prod, \n",
    "                      num_boost_round = nrounds ,\n",
    "                      feval = eval_error,\n",
    "                      #maximize = True,\n",
    "                      #early_stopping_rounds = 10,\n",
    "                      )\n",
    "\n",
    "XGB_prediction = xgb_model.predict(dtest_prod)\n",
    "\n",
    "sub_XGB= pd.DataFrame({'inst_id' : sub_id , 'OC' : XGB_prediction })\n",
    "sub_XGB= sub_XGB[['inst_id', 'OC']]\n",
    "  \n",
    "#PLotting the feature importance\n",
    "xgb_Imp = pd.DataFrame({'Features' : list(xgb_model.get_score().keys()), \n",
    "                        'Importance' : list(xgb_model.get_score().values())}).sort_values(['Importance'])\n",
    "plt.figure()\n",
    "sns.barplot(x=xgb_Imp.Importance, y=xgb_Imp.Features)\n",
    "plt.show()\n",
    "############################################################################\n",
    "#Ensembling the three models\n",
    "############################################################################\n",
    "\n",
    "# #Forming the ensemble dataset of the 3 models\n",
    "ensemble = pd.DataFrame()\n",
    "ensemble['inst_id'] = sub_XGB['inst_id']\n",
    "ensemble['XGB'] = sub_XGB['OC']\n",
    "ensemble['GBM'] = sub_GBM['OC']\n",
    "ensemble['RF'] = sub_RF['OC']\n",
    "\n",
    "# 훈련 데이터에 대한 예측값 얻기\n",
    "RF_train_prediction = RF_prod.predict_proba(train_prod_X)[:, 1]\n",
    "GBM_train_prediction = GBM_prod.predict_proba(train_prod_X)[:, 1]\n",
    "XGB_train_prediction = xgb_model.predict(dtrain_prod)\n",
    "\n",
    "# 훈련 데이터에 대한 앙상블 예측값 생성\n",
    "ensemble_train = pd.DataFrame({\n",
    "    'XGB': XGB_train_prediction,\n",
    "    'GBM': GBM_train_prediction,\n",
    "    'RF': RF_train_prediction\n",
    "})\n",
    "ensemble_train['ens'] = (ensemble_train['XGB'] + ensemble_train['GBM'] + ensemble_train['RF']) / 3\n",
    "\n",
    "# 최적의 임계값 찾기\n",
    "best_thr_ensemble = best_threshold_f1(ensemble_train['ens'], train_prod_Y)\n",
    "\n",
    "# 테스트 데이터에 대한 앙상블 예측값 생성\n",
    "ensemble = pd.DataFrame({\n",
    "    'inst_id': sub_XGB['inst_id'],\n",
    "    'XGB': sub_XGB['OC'],\n",
    "    'GBM': sub_GBM['OC'],\n",
    "    'RF': sub_RF['OC']\n",
    "})\n",
    "ensemble['ens'] = (ensemble['XGB'] + ensemble['GBM'] + ensemble['RF']) / 3\n",
    "\n",
    "# 최적의 임계값을 테스트 데이터에 대한 앙상블 예측값에 적용\n",
    "ensemble['OC'] = (ensemble['ens'] > best_thr_ensemble).astype('int')\n",
    "\n",
    "# 출력하여 확인\n",
    "print(ensemble.loc[ensemble['OC'] == 0, ])\n",
    "\n",
    "ensemble = ensemble[['inst_id', 'OC']]\n",
    "\n",
    "# # Taking the average of all 3 models\n",
    "# ensemble['ens'] = (ensemble['XGB'] + ensemble['GBM'] + ensemble['RF'])/3\n",
    "# # 앙상블 예측값에 대한 최적의 threshold 찾기\n",
    "# best_thr_ensemble = best_threshold_f1(ensemble['ens'], train_prod_Y)  # train_prod_Y를 실제 라벨 값으로 사용\n",
    "\n",
    "# # 최적의 threshold를 사용하여 앙상블 예측값을 이진 분류로 변환\n",
    "# ensemble['OC'] = (ensemble['ens'] > best_thr_ensemble).astype('int')\n",
    "\n",
    "# # 출력하여 확인\n",
    "# print(ensemble.loc[ensemble['OC'] == 0, ])\n",
    "\n",
    "# ensemble = ensemble.loc[:, ['inst_id', 'OC']]\n",
    "\n",
    "ensemble.to_csv('ens.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_640211/1028909028.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  return train_prod_df.append(test_prod_df)\n",
      "/tmp/ipykernel_640211/1028909028.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['OC'] = train_data['OC'] - 1\n",
      "[I 2023-08-24 09:47:20,573] A new study created in memory with name: no-name-0b6ee4aa-fcce-4ba1-9d25-a7fb743b3a94\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:20,922] Trial 0 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 75, 'max_depth': 3, 'min_samples_split': 13, 'min_samples_leaf': 16, 'max_features': 'auto'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:21,545] Trial 1 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 148, 'max_depth': 23, 'min_samples_split': 14, 'min_samples_leaf': 16, 'max_features': 'log2'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:22,037] Trial 2 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 116, 'max_depth': 2, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:22,240] Trial 3 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 45, 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 16, 'max_features': 'log2'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:22,498] Trial 4 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 59, 'max_depth': 2, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:22,588] Trial 5 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 16, 'max_depth': 4, 'min_samples_split': 14, 'min_samples_leaf': 6, 'max_features': 'auto'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:22,990] Trial 6 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 92, 'max_depth': 2, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:23,603] Trial 7 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 145, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 14, 'max_features': 'log2'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:24,146] Trial 8 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 125, 'max_depth': 16, 'min_samples_split': 5, 'min_samples_leaf': 15, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:24,602] Trial 9 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 104, 'max_depth': 19, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:24,968] Trial 10 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 74, 'max_depth': 4, 'min_samples_split': 11, 'min_samples_leaf': 11, 'max_features': 'auto'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:25,631] Trial 11 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 142, 'max_depth': 32, 'min_samples_split': 16, 'min_samples_leaf': 11, 'max_features': 'auto'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:26,002] Trial 12 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 77, 'max_depth': 9, 'min_samples_split': 11, 'min_samples_leaf': 13, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:26,205] Trial 13 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 37, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 11, 'max_features': 'auto'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:26,664] Trial 14 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 97, 'max_depth': 4, 'min_samples_split': 16, 'min_samples_leaf': 16, 'max_features': 'auto'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:26,966] Trial 15 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 62, 'max_depth': 31, 'min_samples_split': 13, 'min_samples_leaf': 13, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:27,553] Trial 16 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 123, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 9, 'max_features': 'auto'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:27,673] Trial 17 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 19, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 13, 'max_features': 'log2'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:28,103] Trial 18 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 88, 'max_depth': 3, 'min_samples_split': 11, 'min_samples_leaf': 9, 'max_features': 'auto'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:28,399] Trial 19 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 61, 'max_depth': 8, 'min_samples_split': 12, 'min_samples_leaf': 15, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:28,925] Trial 20 finished with value: 0.9761199023326816 and parameters: {'n_estimators': 112, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "[I 2023-08-24 09:47:29,542] Trial 21 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 134, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "[I 2023-08-24 09:47:30,042] Trial 22 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 110, 'max_depth': 3, 'min_samples_split': 13, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "[I 2023-08-24 09:47:30,655] Trial 23 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 134, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "[I 2023-08-24 09:47:31,312] Trial 24 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 150, 'max_depth': 7, 'min_samples_split': 14, 'min_samples_leaf': 12, 'max_features': 'log2'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:31,802] Trial 25 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 104, 'max_depth': 12, 'min_samples_split': 13, 'min_samples_leaf': 16, 'max_features': 'auto'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "[I 2023-08-24 09:47:32,199] Trial 26 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 84, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "[I 2023-08-24 09:47:32,760] Trial 27 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 125, 'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "[I 2023-08-24 09:47:33,117] Trial 28 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 75, 'max_depth': 24, 'min_samples_split': 10, 'min_samples_leaf': 14, 'max_features': 'sqrt'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 09:47:33,664] Trial 29 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 115, 'max_depth': 13, 'min_samples_split': 12, 'min_samples_leaf': 10, 'max_features': 'auto'}. Best is trial 20 with value: 0.9761199023326816.\n",
      "[I 2023-08-24 09:47:33,852] A new study created in memory with name: no-name-de1855e6-82d3-404e-82a6-72df536ca4a7\n",
      "[I 2023-08-24 09:47:36,575] Trial 0 finished with value: 0.9705150362062872 and parameters: {'n_estimators': 133, 'max_depth': 27, 'learning_rate': 0.2345413438076932, 'subsample': 0.8363820025832849, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.9705150362062872.\n",
      "[I 2023-08-24 09:47:36,843] Trial 1 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 35, 'max_depth': 6, 'learning_rate': 0.01477435258900461, 'subsample': 0.5116564128444381, 'min_samples_split': 13, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:37,726] Trial 2 finished with value: 0.9370478632606426 and parameters: {'n_estimators': 134, 'max_depth': 3, 'learning_rate': 0.10969761116300618, 'subsample': 0.6826776641567478, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:38,083] Trial 3 finished with value: 0.6856520703616753 and parameters: {'n_estimators': 24, 'max_depth': 6, 'learning_rate': 0.08010219578987765, 'subsample': 0.9888958285894126, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:38,738] Trial 4 finished with value: 0.961253624131576 and parameters: {'n_estimators': 91, 'max_depth': 11, 'learning_rate': 0.2637516134982742, 'subsample': 0.6688466501361494, 'min_samples_split': 8, 'min_samples_leaf': 16}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:39,880] Trial 5 finished with value: 0.8095897328003366 and parameters: {'n_estimators': 95, 'max_depth': 8, 'learning_rate': 0.07015657463660574, 'subsample': 0.61279098433825, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:40,394] Trial 6 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 44, 'max_depth': 30, 'learning_rate': 0.02234688016458295, 'subsample': 0.6626200908601309, 'min_samples_split': 15, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:42,524] Trial 7 finished with value: 0.970380141953746 and parameters: {'n_estimators': 140, 'max_depth': 11, 'learning_rate': 0.1865541969965884, 'subsample': 0.7349898859669427, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:43,021] Trial 8 finished with value: 0.9704962216482954 and parameters: {'n_estimators': 99, 'max_depth': 2, 'learning_rate': 0.07803535744119755, 'subsample': 0.6612355365748476, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:43,834] Trial 9 finished with value: 0.9631729347465389 and parameters: {'n_estimators': 108, 'max_depth': 10, 'learning_rate': 0.27023507784754325, 'subsample': 0.7010060243774264, 'min_samples_split': 3, 'min_samples_leaf': 16}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:44,187] Trial 10 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 58, 'max_depth': 4, 'learning_rate': 0.023308948976707766, 'subsample': 0.5053611611341695, 'min_samples_split': 15, 'min_samples_leaf': 11}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:44,535] Trial 11 finished with value: 0.9726834762158432 and parameters: {'n_estimators': 37, 'max_depth': 30, 'learning_rate': 0.03666367560815152, 'subsample': 0.5283550614727419, 'min_samples_split': 15, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:45,089] Trial 12 finished with value: 0.9709029445490875 and parameters: {'n_estimators': 57, 'max_depth': 20, 'learning_rate': 0.02503847690654301, 'subsample': 0.5938484913678213, 'min_samples_split': 12, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:45,660] Trial 13 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 54, 'max_depth': 17, 'learning_rate': 0.011971878638899059, 'subsample': 0.5577303452253972, 'min_samples_split': 13, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:45,823] Trial 14 finished with value: 0.967285948283636 and parameters: {'n_estimators': 19, 'max_depth': 6, 'learning_rate': 0.14325933946672667, 'subsample': 0.5024737792041098, 'min_samples_split': 16, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:46,346] Trial 15 finished with value: 0.9707763496797966 and parameters: {'n_estimators': 41, 'max_depth': 17, 'learning_rate': 0.04390879479960054, 'subsample': 0.5884503033024181, 'min_samples_split': 12, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:47,173] Trial 16 finished with value: 0.9688083574032381 and parameters: {'n_estimators': 75, 'max_depth': 32, 'learning_rate': 0.12232319888080823, 'subsample': 0.7862529773555065, 'min_samples_split': 13, 'min_samples_leaf': 12}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:47,291] Trial 17 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 11, 'max_depth': 4, 'learning_rate': 0.0651679012350442, 'subsample': 0.6156359761822057, 'min_samples_split': 10, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:48,073] Trial 18 finished with value: 0.9672138221010401 and parameters: {'n_estimators': 72, 'max_depth': 14, 'learning_rate': 0.05083803187265179, 'subsample': 0.5724377262366988, 'min_samples_split': 14, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:48,351] Trial 19 finished with value: 0.9689576668470603 and parameters: {'n_estimators': 40, 'max_depth': 22, 'learning_rate': 0.10637017532567125, 'subsample': 0.5485293712979529, 'min_samples_split': 16, 'min_samples_leaf': 14}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:48,636] Trial 20 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 26, 'max_depth': 8, 'learning_rate': 0.010455321361470316, 'subsample': 0.6296572573167817, 'min_samples_split': 11, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:49,007] Trial 21 finished with value: 0.9726834762158432 and parameters: {'n_estimators': 55, 'max_depth': 5, 'learning_rate': 0.034788864441693705, 'subsample': 0.5193281279691727, 'min_samples_split': 14, 'min_samples_leaf': 11}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:49,357] Trial 22 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 59, 'max_depth': 4, 'learning_rate': 0.019199260846718406, 'subsample': 0.5059604488264614, 'min_samples_split': 15, 'min_samples_leaf': 13}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:49,719] Trial 23 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 45, 'max_depth': 7, 'learning_rate': 0.045132908610584346, 'subsample': 0.5505336039593369, 'min_samples_split': 14, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:50,087] Trial 24 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 66, 'max_depth': 3, 'learning_rate': 0.010581714322598853, 'subsample': 0.5004025393344631, 'min_samples_split': 16, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:50,487] Trial 25 finished with value: 0.9671953970435534 and parameters: {'n_estimators': 32, 'max_depth': 14, 'learning_rate': 0.08909200269974688, 'subsample': 0.5632512076384205, 'min_samples_split': 12, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:51,310] Trial 26 finished with value: 0.9689576668470603 and parameters: {'n_estimators': 84, 'max_depth': 9, 'learning_rate': 0.05861002632519993, 'subsample': 0.6464262288987076, 'min_samples_split': 15, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:52,116] Trial 27 finished with value: 0.972576087899692 and parameters: {'n_estimators': 112, 'max_depth': 6, 'learning_rate': 0.045311607694516254, 'subsample': 0.5968171385316197, 'min_samples_split': 13, 'min_samples_leaf': 12}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:52,566] Trial 28 finished with value: 0.9689576668470603 and parameters: {'n_estimators': 46, 'max_depth': 25, 'learning_rate': 0.09263955364015937, 'subsample': 0.5435621157802559, 'min_samples_split': 11, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:52,784] Trial 29 finished with value: 0.9744457460193501 and parameters: {'n_estimators': 12, 'max_depth': 25, 'learning_rate': 0.06104125555383719, 'subsample': 0.6349014970415796, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.9744457460193501.\n",
      "[I 2023-08-24 09:47:52,911] A new study created in memory with name: no-name-dec13699-3a40-4454-8157-508f3bfc0379\n",
      "[W 2023-08-24 09:47:52,912] Trial 0 failed with parameters: {} because of the following error: NameError(\"name 'xgb' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_640211/1028909028.py\", line 154, in <lambda>\n",
      "    study_xgb.optimize(lambda trial: objective_for_xgb(trial, train_prod_X, train_prod_Y), n_trials=30)\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_640211/1028909028.py\", line 79, in objective_for_xgb\n",
      "    dtrain = xgb.DMatrix(train_X, label=train_Y)\n",
      "             ^^^\n",
      "NameError: name 'xgb' is not defined\n",
      "[W 2023-08-24 09:47:52,914] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 183\u001b[0m\n\u001b[1;32m    177\u001b[0m     submission \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[1;32m    178\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minst_id\u001b[39m\u001b[39m'\u001b[39m: ensemble[\u001b[39m'\u001b[39m\u001b[39minst_id\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mOC\u001b[39m\u001b[39m'\u001b[39m: ensemble[\u001b[39m'\u001b[39m\u001b[39mOC\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    180\u001b[0m     })\n\u001b[1;32m    181\u001b[0m     submission\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39msubmission.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 183\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m# XGBoost Hyperparameter Tuning with Optuna\u001b[39;00m\n\u001b[1;32m    153\u001b[0m study_xgb \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m study_xgb\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective_for_xgb(trial, train_prod_X, train_prod_Y), n_trials\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m    155\u001b[0m best_xgb_params \u001b[39m=\u001b[39m study_xgb\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    156\u001b[0m dtrain \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mDMatrix(train_prod_X, label\u001b[39m=\u001b[39mtrain_prod_Y)\n",
      "File \u001b[0;32m~/anaconda3/envs/hospital/lib/python3.11/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     _optimize(\n\u001b[1;32m    444\u001b[0m         study\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m    445\u001b[0m         func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39mn_trials,\n\u001b[1;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39mn_jobs,\n\u001b[1;32m    449\u001b[0m         catch\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(catch) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(catch, Iterable) \u001b[39melse\u001b[39;00m (catch,),\n\u001b[1;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39mgc_after_trial,\n\u001b[1;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/hospital/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hospital/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/hospital/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/hospital/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[1], line 154\u001b[0m, in \u001b[0;36mmain.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39m# XGBoost Hyperparameter Tuning with Optuna\u001b[39;00m\n\u001b[1;32m    153\u001b[0m study_xgb \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m study_xgb\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective_for_xgb(trial, train_prod_X, train_prod_Y), n_trials\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m    155\u001b[0m best_xgb_params \u001b[39m=\u001b[39m study_xgb\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    156\u001b[0m dtrain \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mDMatrix(train_prod_X, label\u001b[39m=\u001b[39mtrain_prod_Y)\n",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mobjective_for_xgb\u001b[0;34m(trial, train_X, train_Y)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective_for_xgb\u001b[39m(trial, train_X, train_Y):\n\u001b[1;32m     77\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Objective function for tuning XGBoost using Optuna.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     dtrain \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mDMatrix(train_X, label\u001b[39m=\u001b[39mtrain_Y)\n\u001b[1;32m     81\u001b[0m     param \u001b[39m=\u001b[39m {\n\u001b[1;32m     82\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msilent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[1;32m     83\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mbinary:logistic\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m: trial\u001b[39m.\u001b[39msuggest_loguniform(\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1e-8\u001b[39m, \u001b[39m1.0\u001b[39m),\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m param[\u001b[39m'\u001b[39m\u001b[39mbooster\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgbtree\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m param[\u001b[39m'\u001b[39m\u001b[39mbooster\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdart\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import optuna\n",
    "\n",
    "def read_and_process_data():\n",
    "    \"\"\"Reads the data, processes it and returns the combined dataframe.\"\"\"\n",
    "    train_prod_df = pd.read_csv('data/train.csv')\n",
    "    test_prod_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "    process_employee_data(test_prod_df)\n",
    "    process_employee_data(train_prod_df)\n",
    "    train_prod_df.OC = train_prod_df.OC.astype('str').str.replace(\" \", \"\")\n",
    "\n",
    "    return train_prod_df.append(test_prod_df)\n",
    "\n",
    "def process_employee_data(df):\n",
    "    \"\"\"Processes the employee columns to convert them into float type.\"\"\"\n",
    "    df.employee1 = df.employee1.astype('str').str.replace(\",\", \"\").astype('float')\n",
    "    df.employee2 = df.employee2.astype('str').str.replace(\",\", \"\").astype('float')\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    \"\"\"Fills the missing values in the dataframe.\"\"\"\n",
    "    df.loc[df.inst_id == 430, ['instkind']] = 'dental_clinic'\n",
    "    df.loc[df.inst_id == 430, ['bedCount']] = 0\n",
    "    df.loc[df.inst_id == 413, ['bedCount']] = -999\n",
    "\n",
    "    factor_columns = df.select_dtypes(include=['object']).columns\n",
    "    numeric_columns = df.columns.difference(factor_columns)\n",
    "    df[factor_columns] = df[factor_columns].fillna('Not_sure')\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(-999)\n",
    "\n",
    "def label_encode(df, factor_columns):\n",
    "    \"\"\"Encodes the categorical columns using Label Encoder.\"\"\"\n",
    "    fac_le = LabelEncoder()\n",
    "    df[factor_columns] = df.loc[:, factor_columns].apply(lambda x: fac_le.fit_transform(x))\n",
    "\n",
    "def split_train_test(df):\n",
    "    \"\"\"Splits the data into training and testing datasets.\"\"\"\n",
    "    train_data = df.loc[df.OC != 0, ]\n",
    "    test_data = df.loc[df.OC == 0, ]\n",
    "    train_data['OC'] = train_data['OC'] - 1\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def objective_for_rf(trial, train_X, train_Y):\n",
    "    \"\"\"Objective function for tuning Random Forest using Optuna.\"\"\"\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 150)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 16)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                   min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                   max_features=max_features)\n",
    "    return np.mean(cross_val_score(model, train_X, train_Y, cv=3, scoring='f1'))\n",
    "\n",
    "def objective_for_gbm(trial, train_X, train_Y):\n",
    "    \"\"\"Objective function for tuning Gradient Boosting using Optuna.\"\"\"\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 150)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32, log=True)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 16)\n",
    "\n",
    "    model = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                       learning_rate=learning_rate, subsample=subsample, \n",
    "                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "    return np.mean(cross_val_score(model, train_X, train_Y, cv=3, scoring='f1'))\n",
    "\n",
    "def objective_for_xgb(trial, train_X, train_Y):\n",
    "    \"\"\"Objective function for tuning XGBoost using Optuna.\"\"\"\n",
    "    \n",
    "    dtrain = xgb.DMatrix(train_X, label=train_Y)\n",
    "    \n",
    "    param = {\n",
    "        'silent': 1,\n",
    "        'objective': 'binary:logistic',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
    "    }\n",
    "\n",
    "    if param['booster'] == 'gbtree' or param['booster'] == 'dart':\n",
    "        param['max_depth'] = trial.suggest_int('max_depth', 1, 9)\n",
    "        param['n_estimators'] = trial.suggest_int('n_estimators', 50, 300)\n",
    "        param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n",
    "        param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "        param['grow_policy'] = trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide'])\n",
    "        \n",
    "    if param['booster'] == 'dart':\n",
    "        param['sample_type'] = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])\n",
    "        param['normalize_type'] = trial.suggest_categorical('normalize_type', ['tree', 'forest'])\n",
    "        param['rate_drop'] = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)\n",
    "        param['skip_drop'] = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)\n",
    "    \n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dtrain)\n",
    "    \n",
    "    pred_labels = np.rint(preds)\n",
    "    return f1_score(train_Y, pred_labels)\n",
    "\n",
    "def find_best_threshold(probs, y_true):\n",
    "    \"\"\"Find the best threshold to maximize F1 Score based on given probabilities.\"\"\"\n",
    "    best_threshold = 0\n",
    "    best_f1 = 0\n",
    "    for threshold in np.linspace(0, 1, 200): # 여러 임계값들을 테스트\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function that prepares data, optimizes the models, and creates the submission file.\"\"\"\n",
    "    train_test_prod = read_and_process_data()\n",
    "    fill_missing_values(train_test_prod)\n",
    "\n",
    "    factor_columns = train_test_prod.select_dtypes(include=['object']).columns\n",
    "    label_encode(train_test_prod, factor_columns)\n",
    "\n",
    "    train_prod, test_prod = split_train_test(train_test_prod)\n",
    "    sub_id = test_prod.inst_id\n",
    "\n",
    "    indep = train_prod.columns.difference(['OC'])\n",
    "    train_prod_X = train_prod[indep]\n",
    "    train_prod_Y = train_prod['OC']\n",
    "    test_prod_X = test_prod[indep]\n",
    "\n",
    "    # RandomForest Hyperparameter Tuning with Optuna\n",
    "    study_rf = optuna.create_study(direction='maximize')\n",
    "    study_rf.optimize(lambda trial: objective_for_rf(trial, train_prod_X, train_prod_Y), n_trials=30)\n",
    "    best_rf_params = study_rf.best_params\n",
    "    rf_best = RandomForestClassifier(**best_rf_params)\n",
    "    rf_best.fit(train_prod_X, train_prod_Y)\n",
    "    RF_predictions = rf_best.predict_proba(test_prod_X)[:, 1]\n",
    "\n",
    "    # GradientBoosting Hyperparameter Tuning with Optuna\n",
    "    study_gbm = optuna.create_study(direction='maximize')\n",
    "    study_gbm.optimize(lambda trial: objective_for_gbm(trial, train_prod_X, train_prod_Y), n_trials=30)\n",
    "    best_gbm_params = study_gbm.best_params\n",
    "    gbm_best = GradientBoostingClassifier(**best_gbm_params)\n",
    "    gbm_best.fit(train_prod_X, train_prod_Y)\n",
    "    GBM_predictions = gbm_best.predict_proba(test_prod_X)[:, 1]\n",
    "    \n",
    "    # XGBoost Hyperparameter Tuning with Optuna\n",
    "    study_xgb = optuna.create_study(direction='maximize')\n",
    "    study_xgb.optimize(lambda trial: objective_for_xgb(trial, train_prod_X, train_prod_Y), n_trials=30)\n",
    "    best_xgb_params = study_xgb.best_params\n",
    "    dtrain = xgb.DMatrix(train_prod_X, label=train_prod_Y)\n",
    "    dtest = xgb.DMatrix(test_prod_X)\n",
    "    xgb_best = xgb.train(best_xgb_params, dtrain)\n",
    "    XGB_predictions = xgb_best.predict(dtest)\n",
    "\n",
    "    # Ensemble of the models\n",
    "    ensemble = pd.DataFrame({\n",
    "        'inst_id': sub_id,\n",
    "        'RF': RF_predictions,\n",
    "        'GBM': GBM_predictions,\n",
    "        'XGB': XGB_predictions\n",
    "    })\n",
    "\n",
    "    ensemble['ens'] = ensemble[['RF', 'GBM', 'XGB']].mean(axis=1)\n",
    "\n",
    "    # Find the best threshold on ensemble probabilities\n",
    "    best_threshold = find_best_threshold(ensemble['ens'], train_prod_Y)\n",
    "\n",
    "    # Apply threshold to the ensemble predictions\n",
    "    ensemble['OC'] = (ensemble['ens'] >= best_threshold).astype(int)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'inst_id': ensemble['inst_id'],\n",
    "        'OC': ensemble['OC']\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hospital",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
