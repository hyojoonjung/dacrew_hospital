{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 10:19:09,499] A new study created in memory with name: no-name-4c80b8c1-c83b-404d-9b04-eb0a4913495d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 10:19:10,097] Trial 0 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 140, 'max_depth': 20, 'min_samples_split': 12, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "[I 2023-08-24 10:19:10,437] Trial 1 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 80, 'max_depth': 25, 'min_samples_split': 6, 'min_samples_leaf': 15, 'max_features': 'log2'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "[I 2023-08-24 10:19:10,908] Trial 2 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 114, 'max_depth': 2, 'min_samples_split': 13, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "[I 2023-08-24 10:19:11,329] Trial 3 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 102, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 13, 'max_features': 'log2'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "[I 2023-08-24 10:19:11,907] Trial 4 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 134, 'max_depth': 28, 'min_samples_split': 13, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 10:19:12,246] Trial 5 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 77, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 14, 'max_features': 'auto'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 10:19:12,650] Trial 6 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 92, 'max_depth': 29, 'min_samples_split': 6, 'min_samples_leaf': 13, 'max_features': 'auto'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 10:19:13,060] Trial 7 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 94, 'max_depth': 17, 'min_samples_split': 3, 'min_samples_leaf': 12, 'max_features': 'auto'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "/home/hjjung113/anaconda3/envs/hospital/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "[I 2023-08-24 10:19:13,491] Trial 8 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 98, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 8, 'max_features': 'auto'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "[I 2023-08-24 10:19:13,560] Trial 9 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 12, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 0 with value: 0.9743589743589743.\n",
      "[I 2023-08-24 10:19:13,560] A new study created in memory with name: no-name-12c43e01-4746-4a5f-bc86-bd02a3c97a9f\n",
      "[I 2023-08-24 10:19:14,977] Trial 0 finished with value: 0.95183590250996 and parameters: {'n_estimators': 120, 'max_depth': 6, 'learning_rate': 0.02038537129469311, 'subsample': 0.969166265678375, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.95183590250996.\n",
      "[I 2023-08-24 10:19:15,481] Trial 1 finished with value: 0.9742486903777227 and parameters: {'n_estimators': 47, 'max_depth': 21, 'learning_rate': 0.03562730878347282, 'subsample': 0.5404929146998232, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.9742486903777227.\n",
      "[I 2023-08-24 10:19:16,120] Trial 2 finished with value: 0.9585758811765004 and parameters: {'n_estimators': 56, 'max_depth': 8, 'learning_rate': 0.052525809366735926, 'subsample': 0.7538251214624698, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.9742486903777227.\n",
      "[I 2023-08-24 10:19:17,220] Trial 3 finished with value: 0.9698079698079698 and parameters: {'n_estimators': 122, 'max_depth': 5, 'learning_rate': 0.17645390330435565, 'subsample': 0.9509017770301992, 'min_samples_split': 12, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.9742486903777227.\n",
      "[I 2023-08-24 10:19:17,880] Trial 4 finished with value: 0.9493646824874618 and parameters: {'n_estimators': 52, 'max_depth': 7, 'learning_rate': 0.29868950901121194, 'subsample': 0.9410776827668734, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.9742486903777227.\n",
      "[I 2023-08-24 10:19:18,578] Trial 5 finished with value: 0.963215433803669 and parameters: {'n_estimators': 113, 'max_depth': 14, 'learning_rate': 0.18052432117940523, 'subsample': 0.5686774822723437, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.9742486903777227.\n",
      "[I 2023-08-24 10:19:18,928] Trial 6 finished with value: 0.9698079698079698 and parameters: {'n_estimators': 65, 'max_depth': 3, 'learning_rate': 0.24558432421174842, 'subsample': 0.7180795784473402, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.9742486903777227.\n",
      "[I 2023-08-24 10:19:19,951] Trial 7 finished with value: 0.9675727322786146 and parameters: {'n_estimators': 121, 'max_depth': 6, 'learning_rate': 0.20184484135536854, 'subsample': 0.76578889829922, 'min_samples_split': 14, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.9742486903777227.\n",
      "[I 2023-08-24 10:19:20,618] Trial 8 finished with value: 0.9374756996206838 and parameters: {'n_estimators': 48, 'max_depth': 9, 'learning_rate': 0.29188794442318416, 'subsample': 0.8214031295135318, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.9742486903777227.\n",
      "[I 2023-08-24 10:19:20,840] Trial 9 finished with value: 0.9743589743589743 and parameters: {'n_estimators': 42, 'max_depth': 3, 'learning_rate': 0.014264761734313586, 'subsample': 0.6659022302996692, 'min_samples_split': 13, 'min_samples_leaf': 9}. Best is trial 9 with value: 0.9743589743589743.\n",
      "[I 2023-08-24 10:19:20,841] A new study created in memory with name: no-name-90869ad6-53bf-48d8-8e2b-7e34e4be28af\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
      "[I 2023-08-24 10:19:20,857] Trial 0 finished with value: 0.978448275862069 and parameters: {'booster': 'gbtree', 'lambda': 0.000997168172764078, 'alpha': 0.004523236455553415, 'max_depth': 2, 'n_estimators': 111, 'learning_rate': 3.409388541486753e-06, 'gamma': 0.0026270943241886737, 'grow_policy': 'depthwise'}. Best is trial 0 with value: 0.978448275862069.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "[I 2023-08-24 10:19:20,868] Trial 1 finished with value: 0.971677559912854 and parameters: {'booster': 'gblinear', 'lambda': 1.6631193039578582e-05, 'alpha': 7.741450443368771e-08}. Best is trial 0 with value: 0.978448275862069.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:98: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['rate_drop'] = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:99: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['skip_drop'] = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)\n",
      "[I 2023-08-24 10:19:20,892] Trial 2 finished with value: 0.9847494553376906 and parameters: {'booster': 'dart', 'lambda': 8.547309416545518e-07, 'alpha': 0.0016602481950030235, 'max_depth': 7, 'n_estimators': 250, 'learning_rate': 6.814110816544242e-05, 'gamma': 6.674973121061517e-07, 'grow_policy': 'depthwise', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 1.8120616378064987e-08, 'skip_drop': 2.6584497662220632e-05}. Best is trial 2 with value: 0.9847494553376906.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "[I 2023-08-24 10:19:20,902] Trial 3 finished with value: 0.6029850746268657 and parameters: {'booster': 'gblinear', 'lambda': 0.31830629705068764, 'alpha': 2.7772510301418523e-06}. Best is trial 2 with value: 0.9847494553376906.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
      "[I 2023-08-24 10:19:20,920] Trial 4 finished with value: 0.9803921568627451 and parameters: {'booster': 'gbtree', 'lambda': 4.127392274437997e-06, 'alpha': 6.807982014414768e-08, 'max_depth': 8, 'n_estimators': 279, 'learning_rate': 0.0014982455047855398, 'gamma': 7.746763868363724e-05, 'grow_policy': 'lossguide'}. Best is trial 2 with value: 0.9847494553376906.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "[I 2023-08-24 10:19:20,930] Trial 5 finished with value: 0.9586056644880173 and parameters: {'booster': 'gblinear', 'lambda': 4.595305899823889e-06, 'alpha': 2.791332686706543e-05}. Best is trial 2 with value: 0.9847494553376906.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "[I 2023-08-24 10:19:20,939] Trial 6 finished with value: 0.9519650655021834 and parameters: {'booster': 'gblinear', 'lambda': 1.3322236149316292e-07, 'alpha': 4.366182387607863e-05}. Best is trial 2 with value: 0.9847494553376906.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
      "[I 2023-08-24 10:19:20,952] Trial 7 finished with value: 0.978448275862069 and parameters: {'booster': 'gbtree', 'lambda': 1.9760898018193285e-05, 'alpha': 5.008531174808604e-05, 'max_depth': 1, 'n_estimators': 291, 'learning_rate': 0.028367889549374213, 'gamma': 7.256259154853251e-07, 'grow_policy': 'depthwise'}. Best is trial 2 with value: 0.9847494553376906.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "[I 2023-08-24 10:19:20,961] Trial 8 finished with value: 0.9826839826839826 and parameters: {'booster': 'gblinear', 'lambda': 0.1285658327065319, 'alpha': 0.1544285142595353}. Best is trial 2 with value: 0.9847494553376906.\n",
      "/tmp/ipykernel_642284/1532198335.py:84: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:85: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
      "/tmp/ipykernel_642284/1532198335.py:91: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:98: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['rate_drop'] = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)\n",
      "/tmp/ipykernel_642284/1532198335.py:99: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  param['skip_drop'] = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)\n",
      "[I 2023-08-24 10:19:20,982] Trial 9 finished with value: 0.9848156182212581 and parameters: {'booster': 'dart', 'lambda': 0.048387533078260866, 'alpha': 1.0206172450130966e-08, 'max_depth': 7, 'n_estimators': 73, 'learning_rate': 0.001253964038423356, 'gamma': 7.366224152428704e-07, 'grow_policy': 'lossguide', 'sample_type': 'weighted', 'normalize_type': 'forest', 'rate_drop': 0.4915846084394973, 'skip_drop': 1.4343240250722024e-08}. Best is trial 9 with value: 0.9848156182212581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\", \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\", \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\", \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\", \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "[10:19:20] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\", \"silent\" } are not used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import optuna\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 1123\n",
    "def set_global_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "set_global_seed(SEED)\n",
    "\n",
    "def read_and_process_data():\n",
    "    \"\"\"Reads and processes the data.\"\"\"\n",
    "    train_df = pd.read_csv('data/train.csv')\n",
    "    test_df = pd.read_csv('data/test.csv')\n",
    "    for df in [train_df, test_df]:\n",
    "        process_employee_data(df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def process_employee_data(df):\n",
    "    \"\"\"Processes the employee columns to convert them into float type.\"\"\"\n",
    "    df.employee1 = df.employee1.astype('str').str.replace(\",\", \"\").astype('float')\n",
    "    df.employee2 = df.employee2.astype('str').str.replace(\",\", \"\").astype('float')\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    \"\"\"Fills the missing values in the dataframe.\"\"\"\n",
    "    df.loc[df.inst_id == 430, ['instkind']] = 'dental_clinic'\n",
    "    df.loc[df.inst_id == 430, ['bedCount']] = 0\n",
    "    df.loc[df.inst_id == 413, ['bedCount']] = -999\n",
    "\n",
    "    factor_columns = df.select_dtypes(include=['object']).columns\n",
    "    numeric_columns = df.columns.difference(factor_columns)\n",
    "    df[factor_columns] = df[factor_columns].fillna('Not_sure')\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(-999)\n",
    "\n",
    "def label_encode(df, factor_columns):\n",
    "    \"\"\"Encodes the categorical columns using Label Encoder.\"\"\"\n",
    "    fac_le = LabelEncoder()\n",
    "    df[factor_columns] = df.loc[:, factor_columns].apply(lambda x: fac_le.fit_transform(x))\n",
    "\n",
    "\n",
    "def objective_for_rf(trial, X, y):\n",
    "    \"\"\"Objective function for tuning Random Forest using Optuna.\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 32, log=True),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 16),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 16),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=SEED)\n",
    "    return np.mean(cross_val_score(model, X, y, cv=3, scoring='f1'))\n",
    "\n",
    "def objective_for_gbm(trial, X, Y):\n",
    "    \"\"\"Objective function for tuning Gradient Boosting using Optuna.\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 32, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 16),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 16)\n",
    "    }\n",
    "\n",
    "\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "    return np.mean(cross_val_score(model, X, Y, cv=3, scoring='f1'))\n",
    "\n",
    "def objective_for_xgb(trial, X, Y):\n",
    "    \"\"\"Objective function for tuning XGBoost using Optuna.\"\"\"\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X, label=Y)\n",
    "    \n",
    "    param = {\n",
    "        'silent': 1,\n",
    "        'objective': 'binary:logistic',\n",
    "        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n",
    "    }\n",
    "\n",
    "    if param['booster'] == 'gbtree' or param['booster'] == 'dart':\n",
    "        param['max_depth'] = trial.suggest_int('max_depth', 1, 9)\n",
    "        param['n_estimators'] = trial.suggest_int('n_estimators', 50, 300)\n",
    "        param['learning_rate'] = trial.suggest_loguniform('learning_rate', 1e-8, 1.0)\n",
    "        param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "        param['grow_policy'] = trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide'])\n",
    "        \n",
    "    if param['booster'] == 'dart':\n",
    "        param['sample_type'] = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])\n",
    "        param['normalize_type'] = trial.suggest_categorical('normalize_type', ['tree', 'forest'])\n",
    "        param['rate_drop'] = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)\n",
    "        param['skip_drop'] = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)\n",
    "    \n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dtrain)\n",
    "    \n",
    "    pred_labels = np.rint(preds)\n",
    "    return f1_score(Y, pred_labels)\n",
    "\n",
    "def find_best_threshold(probs, y_true):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    for threshold in np.linspace(0, 1, 200): \n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold\n",
    "\n",
    "def main():\n",
    "    # Load and process data\n",
    "    train_data, test_data = read_and_process_data()\n",
    "    \n",
    "    # Fill missing values\n",
    "    fill_missing_values(train_data)\n",
    "    fill_missing_values(test_data)\n",
    "    \n",
    "    # Process employee columns\n",
    "    process_employee_data(train_data)\n",
    "    process_employee_data(test_data)\n",
    "\n",
    "    # Define columns that need to be label encoded\n",
    "    factor_columns = train_data.select_dtypes(include=['object']).columns\n",
    "    label_encode(train_data, factor_columns)\n",
    "    label_encode(test_data, factor_columns)\n",
    "\n",
    "    # Extract features and target variable\n",
    "    X = train_data.drop(columns=['OC'])\n",
    "    y = train_data['OC']\n",
    "    \n",
    "    # Split data into training and validation sets\n",
    "    train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    # Optuna optimization for RandomForest\n",
    "    study_rf = optuna.create_study(direction='maximize')\n",
    "    study_rf.optimize(lambda trial: objective_for_rf(trial, train_X, train_y), n_trials=10)\n",
    "\n",
    "    # Optuna optimization for GradientBoosting\n",
    "    study_gbm = optuna.create_study(direction='maximize')\n",
    "    study_gbm.optimize(lambda trial: objective_for_gbm(trial, train_X, train_y), n_trials=10)\n",
    "\n",
    "    # Optuna optimization for XGBoost\n",
    "    study_xgb = optuna.create_study(direction='maximize')\n",
    "    study_xgb.optimize(lambda trial: objective_for_xgb(trial, train_X, train_y), n_trials=10)\n",
    "\n",
    "    # Train models with optimized hyperparameters\n",
    "    RF_model = RandomForestClassifier(**study_rf.best_params, random_state=SEED).fit(train_X, train_y)\n",
    "    GBM_model = GradientBoostingClassifier(**study_gbm.best_params).fit(train_X, train_y)\n",
    "    XGB_model = xgb.XGBClassifier(**study_xgb.best_params).fit(train_X, train_y)\n",
    "\n",
    "    # Ensemble predictions on validation set\n",
    "    ensemble_val = pd.DataFrame({\n",
    "        'RF': RF_model.predict_proba(val_X)[:, 1],\n",
    "        'GBM': GBM_model.predict_proba(val_X)[:, 1],\n",
    "        'XGB': XGB_model.predict_proba(val_X)[:, 1]\n",
    "    })\n",
    "    ensemble_val['ens'] = ensemble_val.mean(axis=1)\n",
    "    best_threshold = find_best_threshold(ensemble_val['ens'], val_y)\n",
    "\n",
    "    # Ensemble predictions on test set\n",
    "    ensemble = pd.DataFrame({\n",
    "        'inst_id': test_data['inst_id'],\n",
    "        'RF': RF_model.predict_proba(test_data.drop(columns=['OC']))[:, 1],\n",
    "        'GBM': GBM_model.predict_proba(test_data.drop(columns=['OC']))[:, 1],\n",
    "        'XGB': XGB_model.predict_proba(test_data.drop(columns=['OC']))[:, 1]\n",
    "    })\n",
    "    ensemble['ens'] = ensemble.mean(axis=1)\n",
    "    ensemble['OC'] = (ensemble['ens'] >= best_threshold).astype(int)\n",
    "\n",
    "    # Save the ensemble predictions\n",
    "    submission = ensemble[['inst_id', 'OC']]\n",
    "    submission.to_csv('ensemble_submission.csv', index=False)\n",
    "    \n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hospital",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
